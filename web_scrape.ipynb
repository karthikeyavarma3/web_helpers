{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = []\n",
    "\n",
    "def scrape(url):\n",
    "    HEADERS = {'User-Agent': '', 'Accept-Language': 'en-US, en;q=0.5'}\n",
    "    payload = {'api_key': '24fd9631bd5a858204ad10ec3682e2f3', 'url': url}\n",
    "    \n",
    "    webpage = requests.get('http://api.scraperapi.com', params=payload, headers=HEADERS)\n",
    "    print(webpage)\n",
    "    \n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    \n",
    "    links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-no-outline'})\n",
    "    \n",
    "    links_list = []\n",
    "    \n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for link in links_list:\n",
    "        product_url = 'https://www.amazon.in' + link\n",
    "        product_data = scrape_amazon_product(product_url)\n",
    "        if product_data:\n",
    "            scraped_data.append(product_data)\n",
    "            # print(scraped_data)\n",
    "            \n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_amazon_product(url):\n",
    "    \n",
    "    payload = {'api_key' : '24fd9631bd5a858204ad10ec3682e2f3' , 'url' : url}\n",
    "    response = requests.get('http://api.scraperapi.com' , params = payload)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract product name\n",
    "        try:\n",
    "            product_name = soup.find('span', {'id': 'productTitle'}).get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            product_name = ''\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        #no_of_reviews\n",
    "        try:\n",
    "            review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "            # revieww = ['good' , 'bad']\n",
    "\n",
    "        except AttributeError:\n",
    "            review_count = \"\"\n",
    "\n",
    "        time.sleep(5)\n",
    "       \n",
    "       \n",
    "        #availability\n",
    "        try:\n",
    "            available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "            available = available.find(\"span\").string.strip()\n",
    "\n",
    "        except AttributeError:\n",
    "            available = \"Not Available\"\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        #price\n",
    "        try:\n",
    "            price_of_product = soup.find('span' , attrs = {'class' : 'a-offscreen'}).text.strip()\n",
    "        except AttributeError:\n",
    "            price_of_product = ''\n",
    "\n",
    "            time.sleep(5)\n",
    "        \n",
    "        # Extract reviews (you may need to adjust this based on the actual HTML structure)\n",
    "        reviews_section = soup.find_all('div', {'class' : 'a-expander-content reviewText review-text-content a-expander-partial-collapse-content'})\n",
    "        reviews = [review.find('span').text.strip() for review in reviews_section]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        product_data = {\n",
    "            'Product Name': product_name,\n",
    "            'no_of_reviews': review_count,\n",
    "            'Availability': available,\n",
    "            'Price' : price_of_product,\n",
    "            'Reviews': reviews,\n",
    "           \n",
    "        }\n",
    "        print('product_fetched')\n",
    "        \n",
    "        return product_data\n",
    "    else:\n",
    "        print(\"Error: Unable to fetch the webpage.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "product_fetched\n"
     ]
    }
   ],
   "source": [
    "#calling the function and passing the product url to fetch data\n",
    "\n",
    "scrape('#paste ur product url here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, I'd be happy to explain the scraping tool and API used in the context of this web scraping task.\n",
    "\n",
    "In our project, I'm using Beautiful Soup for HTML parsing and data extraction, and I'm also using Scraper API to assist with handling dynamic web pages and potential restrictions.\n",
    "\n",
    "Beautiful Soup:\n",
    "Beautiful Soup is a Python library that is widely used for web scraping and parsing HTML and XML documents. It provides a convenient way to navigate and search the parsed content of a web page.\n",
    "\n",
    "In this task, Beautiful Soup is used to parse the HTML content of the Amazon product pages. We use its methods like find(), find_all(), and attribute access (get_text()) to locate and extract the desired information, such as product name, category, description, reviews, and specifications. Beautiful Soup helps you navigate the HTML structure and extract relevant data points efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper API:\n",
    "Scraper API is a tool that simplifies web scraping by acting as an intermediary between your scraper and the target website. It helps overcome challenges like anti-scraping mechanisms, CAPTCHAs, and dynamic content loading. Scraper API provides you with a simplified way to make HTTP requests to websites, and it handles potential roadblocks that might occur during the scraping process.\n",
    "\n",
    "Usage in Your Project:\n",
    "In this task, Scraper API is used to fetch the HTML content of Amazon product pages. It ensures that the content returned is fully rendered, allowing you to access the data that might be loaded dynamically through JavaScript or AJAX requests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
